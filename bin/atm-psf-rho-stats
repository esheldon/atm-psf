#!/usr/bin/env python


def calc_rho_stats(
    ra, dec,
    T,
    g1, g2,
    weights,
    psf_T,
    psf_g1, psf_g2,
    nproc=1,
    npatch=None,
    min_sep=0.5,
    max_sep=50,
    bin_size=0.2,
    file_name=None,
    model_properties=None,
    low_mem=False,
    **kwargs,
):
    import treecorr

    # treecorr.set_max_omp_threads(1)

    tckwargs = kwargs
    tckwargs['min_sep'] = min_sep
    tckwargs['max_sep'] = max_sep
    tckwargs['bin_size'] = bin_size
    # tckwargs['var_method'] = 'jackknife'
    tckwargs['num_threads'] = nproc
    # tckwargs['verbose'] = 2  # show some progress
    tckwargs['var_method'] = 'bootstrap'

    if 'sep_units' not in tckwargs:
        tckwargs['sep_units'] = 'arcmin'

    # Set this to true if there is a problem and we need to skip plots.
    # skip = False

    # get the shapes
    print(f'using {ra.size} stars')

    dT = T - psf_T
    dg1 = g1 - psf_g1
    dg2 = g2 - psf_g2

    # make the treecorr catalogs
    print("creating Treecorr Catalogs")

    print('    cat_g')
    cat_g = treecorr.Catalog(
        ra=ra, dec=dec,
        ra_units='deg', dec_units='deg',
        g1=g1, g2=g2,
        w=weights,
        npatch=npatch,
    )
    print('    cat_dg')
    cat_dg = treecorr.Catalog(
        ra=ra, dec=dec,
        ra_units='deg', dec_units='deg',
        g1=dg1, g2=dg2,
        w=weights,
        patch_centers=cat_g.patch_centers,
    )
    print('    cat_gdTT')
    cat_gdTT = treecorr.Catalog(
        ra=ra, dec=dec,
        ra_units='deg', dec_units='deg',
        g1=g1 * dT / T, g2=g2 * dT / T,
        w=weights,
        patch_centers=cat_g.patch_centers,
    )

    # setup and run the correlations
    print("doing rho stats")

    # save the rho objects
    data = {}
    print('    rho1')
    data['rho1'] = treecorr.GGCorrelation(tckwargs)
    data['rho1'].process(cat_dg, low_mem=low_mem)

    print('    rho2')
    data['rho2'] = treecorr.GGCorrelation(tckwargs)
    data['rho2'].process(cat_g, cat_dg, low_mem=low_mem)

    print('    rho3')
    data['rho3'] = treecorr.GGCorrelation(tckwargs)
    data['rho3'].process(cat_gdTT, low_mem=low_mem)

    print('    rho4')
    data['rho4'] = treecorr.GGCorrelation(tckwargs)
    data['rho4'].process(cat_dg, cat_gdTT, low_mem=low_mem)

    print('    rho5')
    data['rho5'] = treecorr.GGCorrelation(tckwargs)
    data['rho5'].process(cat_g, cat_gdTT, low_mem=low_mem)
    # treecorr.set_max_omp_threads(None)

    return data


def get_flist(fname):
    flist = []
    print('getting file list from:', fname)
    with open(fname) as fobj:
        for line in fobj:
            flist.append(line.strip())
    return flist


def read_one(inputs):
    import numpy as np
    import esutil as eu
    import fitsio

    fname, nstar_min, seeing_min = inputs

    columns = [
        'ra', 'dec',
        'am_T',
        # 'am_T_err',
        'am_e1',
        # 'am_e1_err',
        'am_e2',
        # 'am_e2_err',
        'am_psf_T', 'am_psf_e1', 'am_psf_e2',
    ]

    try:
        data = fitsio.read(fname)
    except OSError:
        return None

    n = data['star_select'].sum()
    if 'am_flags' in data.dtype.names and n >= nstar_min:

        w, = np.where(
            data['reserved']
            # data['star_select']
            # & (~data['reserved'])
            & (data['am_flags'] == 0)
            & (data['am_psf_flags'] == 0)
        )

        data = data[w]

        all_seeing = np.sqrt(data['am_psf_T'] / 2) * 2.3548
        seeing = np.median(all_seeing)
        if seeing < seeing_min:
            data = None
        else:
            if 'am_flags' in data.dtype.names:
                data = eu.numpy_util.extract_fields(data, columns)
    else:
        data = None

    return data


def read_data(args):
    import esutil as eu
    from tqdm import tqdm
    import multiprocessing
    import fitsio

    if args.fname is not None:
        return fitsio.read(args.fname)
    elif args.flist is not None:
        flist = get_flist(args.flist)
    else:
        raise RuntimeError('send --fname or --flist')

    ncols = 70
    mininterval = 0.5
    if args.nproc != 1:
        inputs = [
            (fname, args.nstar_min, args.seeing_min) for fname in flist
        ]
        pool = multiprocessing.Pool(args.nproc)

        dlist = []
        for data in tqdm(
            pool.imap_unordered(read_one, inputs),
            total=len(inputs),
            ascii=True,
            ncols=ncols,
            mininterval=mininterval,
        ):
            if data is not None:
                dlist.append(data)

    else:
        dlist = []
        for f in tqdm(
            flist, ncols=ncols, ascii=True, mininterval=mininterval,
        ):
            data = read_one((f, args.nstar_min, args.seeing_min))
            if data is not None:
                dlist.append(data)

    print(f'kept {len(dlist)}/{len(flist)}')
    return eu.numpy_util.combine_arrlist(dlist)


def get_args():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--front', required=True,
                        help='front for file names')

    parser.add_argument('--flist', help='file holding list of inputs')

    parser.add_argument(
        '--fname', help='single file with data',
    )

    parser.add_argument(
        '--save-and-exit', action='store_true',
        help='save read data to a file and exit',
    )
    parser.add_argument(
        '--low-mem', action='store_true',
        help='use low_mem mode',
    )

    parser.add_argument(
        '--nstar-min',
        type=int, default=50,
        help='only use images with at least this many stars, default 50'
    )
    parser.add_argument(
        '--nproc', type=int, default=1,
        help='number of processes to use, default 1'
    )
    parser.add_argument(
        '--npatch', type=int, default=40,
        help='number of patches for bootstrap, default 40'
    )

    parser.add_argument(
        '--min-sep', type=float, default=0.5,
        help='minimum separation, default 0.5 arcmin'
    )
    parser.add_argument(
        '--max-sep', type=float, default=50,
        help='maximum separation, default 50 arcmin'
    )

    parser.add_argument(
        '--seeing-min', type=float, default=0,
        help='min seeing, default 0'
    )

    return parser.parse_args()


def main():
    args = get_args()

    print('reading data')
    st = read_data(args)

    if args.save_and_exit:
        import fitsio
        oname = args.front+'-objdata.fits'
        print('writing:', oname)
        fitsio.write(oname, st, clobber=True)
        return

    # note e1/e2 get factor of 1/2 to convert to reduced shear style
    if 'am_e1' in st.dtype.names:
        g1 = st['am_e1'] * 0.5
        g2 = st['am_e2'] * 0.5
        T = st['am_T']
        psf_g1 = st['am_psf_e1'] * 0.5
        psf_g2 = st['am_psf_e2'] * 0.5
        psf_T = st['am_psf_T']

        # g_err2 = (
        #     0.0002**2
        #     + (st['am_e1_err'] * 0.5) ** 2 + (st['am_e2_err'] * 0.5) ** 2
        # )
        # weights = 1 / g_err2
        weights = None
        # weights = 1/st['am_T_err'] ** 2
        # weights = (st['am_T'] /st['am_T_err']) ** 2
    else:
        g1 = st['g1_data']
        g2 = st['g2_data']
        T = st['T_data']
        psf_g1 = st['g1_model']
        psf_g2 = st['g2_model']
        psf_T = st['T_model']

    data = calc_rho_stats(
        ra=st['ra'],
        dec=st['dec'],
        g1=g1,
        g2=g2,
        weights=weights,
        T=T,
        psf_g1=psf_g1,
        psf_g2=psf_g2,
        psf_T=psf_T,
        npatch=args.npatch,
        min_sep=args.min_sep,
        max_sep=args.max_sep,
        nproc=args.nproc,
        low_mem=args.low_mem,
    )

    for key in data:
        fname = args.front + f'-{key}.fits'
        print(f'writing: {fname}')
        data[key].write(fname)


main()
